{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91435066-50fc-4449-ab26-11854f9b92fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate rouge_score bert_score wandb sentencepiece accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba935d7-7eee-452b-8a4a-d69a51890d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/bdr8ftmj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7bb79c185250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from bert_score import score as bert_score\n",
    "import torch\n",
    "from transformers import EarlyStoppingCallback\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\")  # ปิด wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca4d0fa-583d-41ed-89c9-3cf9bb562794",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./train_set_qsum.csv\"\n",
    "val_path = \"./val_set_qsum.csv\"\n",
    "\n",
    "# Load datasets using the datasets library\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": train_path, \"validation\": val_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a42dbc5-752a-47d6-ace1-f1fe42e10cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaa864c18334e38982847314cf69b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102c91f2d3474efb95b8a899c75d3365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer (using LED-large-16384)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "\n",
    "# Set maximum lengths and batch size\n",
    "# encoder_max_length = 8192   # adjust as needed based on your inputs\n",
    "encoder_max_length = 512 \n",
    "decoder_max_length = 128    # adjust as needed for outputs\n",
    "batch_size = 2\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "    # Tokenize the inputs and targets from your dataset columns\n",
    "    inputs = tokenizer(\n",
    "        batch[\"clean_input\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"clean_output\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=decoder_max_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # Create a global attention mask (required by LED):\n",
    "    # Each sample gets a list of zeros with the first token set to 1.\n",
    "    batch_size_local = len(batch[\"input_ids\"])\n",
    "    batch[\"global_attention_mask\"] = [\n",
    "        [0] * encoder_max_length for _ in range(batch_size_local)\n",
    "    ]\n",
    "    for i in range(batch_size_local):\n",
    "        batch[\"global_attention_mask\"][i][0] = 1\n",
    "\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "    # Replace padding token id's in labels by -100 so that they are ignored during loss computation\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        for label in batch[\"labels\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "# Map the processing function onto your datasets and remove the original text columns\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "val_dataset = dataset[\"validation\"].map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "# Set the dataset format to PyTorch tensors for the required columns\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"]\n",
    ")\n",
    "val_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25b5f3f-7a26-4a30-a5e8-9d91d286e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Seq2Seq training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=30,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,                   # enable mixed precision training if supported\n",
    "    output_dir=\"./led_qmsum_results\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=250,\n",
    "    save_steps=1000,\n",
    "    warmup_steps=1500,\n",
    "    metric_for_best_model=\"eval_loss\" ,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "# # Compute ROUGE score during evaluation\n",
    "# def compute_metrics(pred):\n",
    "#     labels_ids = pred.label_ids\n",
    "#     pred_ids = pred.predictions\n",
    "\n",
    "#     # Decode predictions and labels\n",
    "#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "#     label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "#     rouge_output = rouge.compute(\n",
    "#         predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "#     )[\"rouge2\"].mid\n",
    "\n",
    "#     return {\n",
    "#         \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "#         \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "#         \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "#     }\n",
    "\n",
    "# Load LED model for sequence-to-sequence generation with gradient checkpointing enabled\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False\n",
    ")\n",
    "\n",
    "# Set generation hyperparameters\n",
    "model.config.num_beams = 4\n",
    "model.config.max_length = decoder_max_length   # Maximum output length\n",
    "model.config.min_length = 10\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "\n",
    "# Instantiate the Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9253cba-e3ff-4671-8cb9-6a12d01f1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8ca04f-6981-4e8c-90c5-07d4392d3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "Input ids are automatically padded from 512 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/4290 14:19 < 26:39, 1.74 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.930700</td>\n",
       "      <td>3.761585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.460900</td>\n",
       "      <td>3.570818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.201200</td>\n",
       "      <td>3.539871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.750700</td>\n",
       "      <td>3.549694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.321000</td>\n",
       "      <td>3.703313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.743500</td>\n",
       "      <td>3.953089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 10, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=3.0120345764160157, metrics={'train_runtime': 859.9336, 'train_samples_per_second': 39.875, 'train_steps_per_second': 4.989, 'total_flos': 4046931996180480.0, 'train_loss': 3.0120345764160157, 'epoch': 10.48951048951049})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb93adf8-8a27-457b-9ea3-67b0a966c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./led-finetuned512-qmsum\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_path = \"./led-finetuned512-qmsum\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab9121e-d9aa-4f32-baf5-5e57dd2c4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LEDTokenizer, LEDForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6516c5c5-f35d-4e35-89b7-6d9276eb4ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e85ad0eef7434eb7cd9a0a001d7678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE scores...\n"
     ]
    }
   ],
   "source": [
    "# Load your test CSV dataset\n",
    "test_path = \"./test_set_qsum.csv\"\n",
    "test_dataset = load_dataset(\"csv\", data_files={\"test\": test_path})[\"test\"]\n",
    "\n",
    "# Load T5 tokenizer and model\n",
    "# model_path = \"./led-finetuned-qmsum\"  # Your saved model path\n",
    "model_path = \"./led_qmsum_results/checkpoint-1000/\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_path)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\n",
    "\n",
    "def infer_led(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Inference function for LED model.\n",
    "    \n",
    "    Args:\n",
    "        input_text (str): The input text string to summarize or answer.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated output text.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs_dict = tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096,  # Adjust max_length as needed\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
    "    \n",
    "    # Create a global attention mask (required for LED)\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    global_attention_mask[:, 0] = 1  # Set global attention on the first token\n",
    "    \n",
    "    # Generate prediction\n",
    "    predicted_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        global_attention_mask=global_attention_mask\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens to text\n",
    "    output_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "\n",
    "def generate_answer_LED(batch):\n",
    "    # Tokenize the input text\n",
    "    inputs_dict = tokenizer(\n",
    "        batch[\"clean_input\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096,  # Adjust as needed for your inputs\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
    "\n",
    "    # Create a global attention mask (required for LED)\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    global_attention_mask[:, 0] = 1  # Set global attention on the first token\n",
    "\n",
    "    # Generate predictions\n",
    "    predicted_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        global_attention_mask=global_attention_mask\n",
    "    )\n",
    "    # Decode the generated tokens to text\n",
    "    batch[\"predicted_output\"] = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "# Map the generation function over the test set (batched for efficiency)\n",
    "results_led = test_dataset.map(generate_answer_LED, batched=True, batch_size=4)\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "print(\"Calculating ROUGE scores...\")\n",
    "rouge_scores = rouge.compute(\n",
    "    predictions=results_led[\"predicted_output\"],\n",
    "    references=results_led[\"clean_output\"],\n",
    "    use_stemmer=True,\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67cecb8-5dfb-4d0b-9f5a-2610db247f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== T5 Evaluation Results =====\n",
      "ROUGE Scores:\n",
      "rouge1: 0.2800\n",
      "rouge2: 0.0608\n",
      "rougeL: 0.1777\n"
     ]
    }
   ],
   "source": [
    "# Print all scores\n",
    "print(\"\\n===== T5 Evaluation Results =====\")\n",
    "print(\"ROUGE Scores:\")\n",
    "for metric, scores in rouge_scores.items():\n",
    "    print(f\"{metric}: {scores:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb979309-df18-436c-8015-35d4a382474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Calculate BERTScore\n",
    "print(\"Calculating BERTScore...\")\n",
    "# If the dataset is large, you might want to limit the number of examples for BERTScore\n",
    "# as it can be computationally intensive\n",
    "max_samples_for_bertscore = 100\n",
    "if len(results_led[\"predicted_output\"]) > max_samples_for_bertscore:\n",
    "    print(f\"Limiting BERTScore calculation to {max_samples_for_bertscore} samples.\")\n",
    "    indices = np.random.choice(len(results_led[\"predicted_output\"]), max_samples_for_bertscore, replace=False)\n",
    "    bertscore_preds = [results_led[\"predicted_output\"][i] for i in indices]\n",
    "    bertscore_refs = [results_led[\"clean_output\"][i] for i in indices]\n",
    "else:\n",
    "    bertscore_preds = results_led[\"predicted_output\"]\n",
    "    bertscore_refs = results_led[\"clean_output\"]\n",
    "\n",
    "P, R, F1 = bert_score(bertscore_preds, bertscore_refs, lang='en', rescale_with_baseline=False)\n",
    "bert_f1 = torch.mean(F1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfc31c8-b3bd-4242-937d-bd0e24f864d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Longformer Evaluation Results =====\n",
      "ROUGE Scores:\n",
      "rouge1: 0.2800\n",
      "rouge2: 0.0608\n",
      "rougeL: 0.1777\n",
      "\n",
      "BERTScore F1: 0.8397\n"
     ]
    }
   ],
   "source": [
    "# Print all scores\n",
    "print(\"\\n===== Longformer Evaluation Results =====\")\n",
    "print(\"ROUGE Scores:\")\n",
    "for metric, scores in rouge_scores.items():\n",
    "    print(f\"{metric}: {scores:.4f}\")\n",
    "print(f\"\\nBERTScore F1: {bert_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77dc943c-a619-47af-bcf7-860e456558ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing text for evaluation...\n",
      "Calculating ROUGE scores...\n",
      "Calculating BERTScore...\n",
      "Limiting BERTScore calculation to 100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# Compute ROUGE scores\n",
    "def postprocess_text(preds, refs):\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in preds]\n",
    "    refs = [\"\\n\".join(sent_tokenize(ref.strip())) for ref in refs]\n",
    "    return preds, refs\n",
    "\n",
    "# Post-process predictions and references\n",
    "print(\"Post-processing text for evaluation...\")\n",
    "processed_preds, processed_refs = postprocess_text(\n",
    "    results_led[\"predicted_output\"],\n",
    "    results_led[\"clean_output\"]\n",
    ")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "print(\"Calculating ROUGE scores...\")\n",
    "rouge_scores = rouge.compute(\n",
    "    predictions=processed_preds,\n",
    "    references=processed_refs,\n",
    "    use_stemmer=True,\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    ")\n",
    "\n",
    "# Calculate BERTScore\n",
    "print(\"Calculating BERTScore...\")\n",
    "# If the dataset is large, you might want to limit the number of examples for BERTScore\n",
    "# as it can be computationally intensive\n",
    "max_samples_for_bertscore = 100\n",
    "if len(processed_preds) > max_samples_for_bertscore:\n",
    "    print(f\"Limiting BERTScore calculation to {max_samples_for_bertscore} samples.\")\n",
    "    indices = np.random.choice(len(processed_preds), max_samples_for_bertscore, replace=False)\n",
    "    bertscore_preds = [processed_preds[i] for i in indices]\n",
    "    bertscore_refs = [processed_refs[i] for i in indices]\n",
    "else:\n",
    "    bertscore_preds = processed_preds\n",
    "    bertscore_refs = processed_refs\n",
    "\n",
    "P, R, F1 = bert_score(bertscore_preds, bertscore_refs, lang='en', rescale_with_baseline=False)\n",
    "bert_f1 = torch.mean(F1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51baded3-4d30-48de-a4b8-02f453fcb2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Longformer Evaluation Results =====\n",
      "ROUGE Scores:\n",
      "rouge1: 0.2101\n",
      "rouge2: 0.0637\n",
      "rougeL: 0.1667\n",
      "\n",
      "BERTScore F1: 0.8414\n"
     ]
    }
   ],
   "source": [
    "# Print all scores\n",
    "print(\"\\n===== Longformer Evaluation Results =====\")\n",
    "print(\"ROUGE Scores:\")\n",
    "for metric, scores in rouge_scores.items():\n",
    "    print(f\"{metric}: {scores:.4f}\")\n",
    "print(f\"\\nBERTScore F1: {bert_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ac0b1-64ba-4107-ad50-42d3df3aa1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
