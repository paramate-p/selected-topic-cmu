{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/paramatephuengtrakul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/paramatephuengtrakul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.data.path.append('/Users/paramatephuengtrakul/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_punctuation: bool):\n",
    "\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII characters\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text) # replace multiple white space with sigle white space\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text) # remove specific char, `Â¬`, or quotes around words\n",
    "\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Remove all punctuation\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    text = text.lower() # convert to lower \n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenized_sentences_words(text):\n",
    "    \n",
    "    text = clean_text(text, remove_punctuation=False)\n",
    "\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "\n",
    "    # remove white space if the particular sentence has white space\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    \n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences] # split by white space\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def count_word_frequency(text):\n",
    "\n",
    "    # tokenize\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    word_freq = {}\n",
    "    \n",
    "\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1 \n",
    "        else:\n",
    "            word_freq[word] = 1 \n",
    "\n",
    "    return word_freq\n",
    "\n",
    "def get_top_k_words(text, k: int):\n",
    "\n",
    "    word_freq = count_word_frequency(text)\n",
    "    \n",
    "    df_top_k = pd.DataFrame(list(word_freq.items()), columns=['Word', 'Frequency'])\n",
    "\n",
    "    # sort by descending order\n",
    "    df_top_k_sorted = df_top_k.sort_values(by='Frequency', ascending=False).head(k)\n",
    "    \n",
    "    return df_top_k_sorted\n",
    "\n",
    "\n",
    "# tokenization across nltk, textblob and spacy\n",
    "def tokenize_nltk(text):\n",
    "    token_list= word_tokenize(text)\n",
    "    return token_list\n",
    "\n",
    "def tokenize_textblob(text):\n",
    "    blob = TextBlob(text)\n",
    "    token_list = blob.words\n",
    "    return token_list\n",
    "\n",
    "def tokenize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    token_list = [token.text for token in doc]\n",
    "    return token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"aliced29.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    texts = file.read()\n",
    "\n",
    "# print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A cleaned version of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'or longitude ive got to alice had no idea what latitude was or longitude either but thought they were nice grand words to say presently she began again i wonder if i shall fall right through the earth how funny itll seem to come out among the people that walk with their heads downward the antipathies i think she was rather glad there was no one listening this time as it didnt sound at all the right word but i shall have to ask them what the name of the country is you know please maam is this new zealand or australia and she tried to curtsey as she spokefancy curtseying as youre falling through the air do you think you could manage it and what an ignorant little girl shell think me for asking no itll never do to ask perhaps i shall see it written up somewhere down down down there was nothing else to do so alice soon began talking again dinahll miss me very much tonight i should think dinah was the cat i hope theyll remember her saucer of milk at teatime dinah my dear i wish you were'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = clean_text(texts, remove_punctuation=True)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of tokenized sentences and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['or', 'longitude', 'ive', 'got', 'to'],\n",
       " ['alice',\n",
       "  'had',\n",
       "  'no',\n",
       "  'idea',\n",
       "  'what',\n",
       "  'latitude',\n",
       "  'was,',\n",
       "  'or',\n",
       "  'longitude',\n",
       "  'either,',\n",
       "  'but',\n",
       "  'thought',\n",
       "  'they',\n",
       "  'were',\n",
       "  'nice',\n",
       "  'grand',\n",
       "  'words',\n",
       "  'to',\n",
       "  'say']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_token = tokenized_sentences_words(texts)\n",
    "sentences_token[:2] # show first 2 elements result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A printed table of the top 10 most frequent words and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>i</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>the</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>was</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>you</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>she</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>think</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>down</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>as</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>shall</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  Frequency\n",
       "25       i          8\n",
       "32     the          8\n",
       "4       to          7\n",
       "11     was          5\n",
       "71     you          4\n",
       "22     she          4\n",
       "49   think          4\n",
       "103   down          3\n",
       "57      as          3\n",
       "28   shall          3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_words(cleaned_text, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/paramatephuengtrakul/nltk_data'\n    - '/Users/paramatephuengtrakul/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/nltk_data'\n    - '/Users/paramatephuengtrakul/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/share/nltk_data'\n    - '/Users/paramatephuengtrakul/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/paramatephuengtrakul/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# start_time = time.time()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# token_txtblob = tokenize_textblob(cleaned_text)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# end_time = time.time()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     'tokens':token_txtblob\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m     19\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 20\u001b[0m token_nltk \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_nltk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     22\u001b[0m result_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution_time_sec\u001b[39m\u001b[38;5;124m'\u001b[39m: end_time \u001b[38;5;241m-\u001b[39m start_time,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m:token_nltk\n\u001b[1;32m     25\u001b[0m }\n",
      "Cell \u001b[0;32mIn[13], line 62\u001b[0m, in \u001b[0;36mtokenize_nltk\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_nltk\u001b[39m(text):\n\u001b[0;32m---> 62\u001b[0m     token_list\u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token_list\n",
      "File \u001b[0;32m~/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m~/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[0;32m~/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[0;32m~/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/paramatephuengtrakul/nltk_data'\n    - '/Users/paramatephuengtrakul/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/nltk_data'\n    - '/Users/paramatephuengtrakul/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/share/nltk_data'\n    - '/Users/paramatephuengtrakul/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/paramatephuengtrakul/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "start_time = time.time()\n",
    "token_spc = tokenize_spacy(cleaned_text)\n",
    "end_time = time.time()\n",
    "result_dict[\"Spacy\"] = {\n",
    "    'execution_time_sec': end_time - start_time,\n",
    "    'tokens':token_spc\n",
    "}\n",
    "\n",
    "# start_time = time.time()\n",
    "# token_txtblob = tokenize_textblob(cleaned_text)\n",
    "# end_time = time.time()\n",
    "# result_dict[\"TextBlob\"] = {\n",
    "#     'execution_time_sec': end_time - start_time,\n",
    "#     'tokens':token_txtblob\n",
    "# }\n",
    "\n",
    "start_time = time.time()\n",
    "token_nltk = tokenize_nltk(cleaned_text)\n",
    "end_time = time.time()\n",
    "result_dict[\"nltk\"] = {\n",
    "    'execution_time_sec': end_time - start_time,\n",
    "    'tokens':token_nltk\n",
    "}\n",
    "\n",
    "all_results = {\n",
    "    'method': [],\n",
    "    'execution_time_sec': [],\n",
    "    'tokens': []\n",
    "}\n",
    "\n",
    "for method, result in result_dict.items():\n",
    "    all_results['method'].append(method)\n",
    "    all_results['execution_time_sec'].append(result['execution_time_sec'])\n",
    "    all_results['tokens'].append(result['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/paramatephuengtrakul/nltk_data', '/Users/paramatephuengtrakul/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/nltk_data', '/Users/paramatephuengtrakul/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/share/nltk_data', '/Users/paramatephuengtrakul/cmu2024/nlp_927/selected-topic-cmu/week1/nlpenv/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/paramatephuengtrakul/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('nlpenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b9bcdc382df04be06aa86b23d9d7b412dd5033c0b20ccee8a1c3ddf75e7b90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
